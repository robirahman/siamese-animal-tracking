{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20319066",
   "metadata": {},
   "source": [
    "# Train Siamese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf7ce8cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T07:23:52.973006Z",
     "start_time": "2022-09-26T07:23:52.859042Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4655299e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T07:30:20.475513Z",
     "start_time": "2022-09-26T07:26:33.202161Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Image' from 'PIL' (C:\\Users\\robir\\.virtualenvs\\lib\\site-packages\\PIL\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Input \u001B[1;32mIn [2]\u001B[0m, in \u001B[0;36m<cell line: 14>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmodel\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msiamese\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel_generator\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m create_model, base_models\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mio\u001B[39;00m\n\u001B[1;32m---> 14\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n",
      "File \u001B[1;32m~\\.virtualenvs\\lib\\site-packages\\matplotlib\\__init__.py:109\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m    105\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpackaging\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mversion\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m parse \u001B[38;5;28;01mas\u001B[39;00m parse_version\n\u001B[0;32m    107\u001B[0m \u001B[38;5;66;03m# cbook must import matplotlib only within function\u001B[39;00m\n\u001B[0;32m    108\u001B[0m \u001B[38;5;66;03m# definitions, so it is safe to import from it here.\u001B[39;00m\n\u001B[1;32m--> 109\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _api, _version, cbook, docstring, rcsetup\n\u001B[0;32m    110\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcbook\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m MatplotlibDeprecationWarning, sanitize_sequence\n\u001B[0;32m    111\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcbook\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m mplDeprecation  \u001B[38;5;66;03m# deprecated\u001B[39;00m\n",
      "File \u001B[1;32m~\\.virtualenvs\\lib\\site-packages\\matplotlib\\rcsetup.py:27\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _api, cbook\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcbook\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ls_mapper\n\u001B[1;32m---> 27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcolors\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Colormap, is_color_like\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfontconfig_pattern\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m parse_fontconfig_pattern\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_enums\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m JoinStyle, CapStyle\n",
      "File \u001B[1;32m~\\.virtualenvs\\lib\\site-packages\\matplotlib\\colors.py:51\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     49\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnumbers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Number\n\u001B[0;32m     50\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mre\u001B[39;00m\n\u001B[1;32m---> 51\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mPIL\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Image\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mPIL\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mPngImagePlugin\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PngInfo\n\u001B[0;32m     54\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mmpl\u001B[39;00m\n",
      "\u001B[1;31mImportError\u001B[0m: cannot import name 'Image' from 'PIL' (C:\\Users\\robir\\.virtualenvs\\lib\\site-packages\\PIL\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# train_siamese.py\n",
    "\n",
    "from datetime import datetime\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from absl import app\n",
    "from tqdm.keras import TqdmCallback\n",
    "\n",
    "from data.data_generator import DataGenerator\n",
    "from model.siamese.config import cfg\n",
    "from model.siamese.model_generator import create_model, base_models\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "train_data_folder = \"data/filter_aug/train\"\n",
    "test_data_folder = \"data/filter_aug/test\"\n",
    "\n",
    "TRAINABLE = True\n",
    "\n",
    "base_model = list(base_models.keys())[0]  # MobileNetV2, ResNet101V2, EfficientNetB5\n",
    "\n",
    "WEIGHTS_DIR = \"model/siamese/weights\"\n",
    "datatype = 'train'\n",
    "\n",
    "#def main(_argv):\n",
    "\n",
    "model = create_model(trainable=TRAINABLE, base_model=base_model)\n",
    "#prefix = \"block3c_add\" # DLiske there is a default configured so I commented this out\n",
    "prefix = '' #DLiske it's used in the filename so i had to keep it\n",
    "\n",
    "try:\n",
    "    tf.keras.utils.plot_model(\n",
    "        model,\n",
    "        to_file=f\"assets/{base_model}_model_fig.png\",\n",
    "        show_shapes=True,\n",
    "        expand_nested=True,\n",
    "    )\n",
    "except ImportError as e:\n",
    "    print(f\"Failed to plot keras model: {e}\")\n",
    "\n",
    "ds_generator = DataGenerator(\n",
    "    file_ext=[\"png\", \"jpg\"],\n",
    "    folder_path=train_data_folder,\n",
    "    exclude_aug=True,\n",
    "    #step_size=4,\n",
    "    step_size=1\n",
    ")\n",
    "\n",
    "# train_ds = ds_generator.get_dataset()\n",
    "\n",
    "learning_rate = cfg.TRAIN.LEARNING_RATE\n",
    "\n",
    "# optimizer = tf.keras.optimizers.RMSprop(lr=learning_rate)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4) #DLiske the orig code had lr instead of learning_rate\n",
    "loss_fun = tfa.losses.TripletSemiHardLoss() # DLiske comment only: The triplets are generated by TF\n",
    "model.compile(loss=loss_fun, optimizer=optimizer, metrics=[])\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    WEIGHTS_DIR + \"/\" + base_model + \"/siam-{epoch}-\"+str(learning_rate)+\"-\"+str(prefix)+\"_{loss:.4f}.h5\",\n",
    "    monitor=\"loss\",\n",
    "    verbose=0,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    mode=\"min\",\n",
    ")\n",
    "stop = tf.keras.callbacks.EarlyStopping(monitor=\"loss\", patience=cfg.TRAIN.PATIENCE, mode=\"min\", restore_best_weights=True)\n",
    "# reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"loss\", factor=0.6, patience=5, min_lr=1e-6, verbose=1,\n",
    "#                                                  mode=\"min\")\n",
    "\n",
    "# Define the Keras TensorBoard callback.\n",
    "logdir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    ds_generator,\n",
    "    #epochs=cfg.TRAIN.EPOCHS,\n",
    "    epochs = 75,\n",
    "    callbacks=[tensorboard_callback, checkpoint, TqdmCallback(verbose=0), stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "#plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "#plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "dataset = ds_generator.get_dataset()\n",
    "results = model.predict(dataset)\n",
    "\n",
    "# save pure results (embedding) and create meta mapping for each row (visualization files)\n",
    "#np.savetxt(f\"vecs-{FLAGS.datatype}-{base_model}.tsv\", results, delimiter=\"\\t\")\n",
    "np.savetxt(f\"vecs-{datatype}-{base_model}.tsv\", results, delimiter=\"\\t\")\n",
    "#out_m = io.open(f\"meta-{FLAGS.datatype}-{base_model}.tsv\", \"w\", encoding=\"utf-8\")\n",
    "out_m = io.open(f\"meta-{datatype}-{base_model}.tsv\", \"w\")#, encoding=\"utf-8\")\n",
    "for img, labels in tfds.as_numpy(dataset):\n",
    "    [out_m.write(str(x) + \"\\n\") for x in labels]\n",
    "out_m.close()\n",
    "\n",
    "# merge all embeddings per class\n",
    "per_class = {}\n",
    "idx = 0\n",
    "for img, labels in tfds.as_numpy(dataset):\n",
    "    for class_id in labels:\n",
    "        if class_id not in per_class:\n",
    "            per_class[class_id] = []\n",
    "        per_class[class_id].append(results[idx])\n",
    "        idx += 1\n",
    "\n",
    "mean_values = None\n",
    "labels = None\n",
    "# calculate average value for each class\n",
    "for class_id, values in per_class.items():\n",
    "    #print(\"CLASS_ID\", class_id)#, values)\n",
    "    matrix = np.array(values)\n",
    "    mean_val = np.mean(matrix, axis=0)\n",
    "    if mean_values is None:\n",
    "        mean_values = np.array([mean_val])\n",
    "    else:\n",
    "        mean_values = np.concatenate((mean_values, np.array([mean_val])), axis=0)\n",
    "    if labels is None:\n",
    "        labels = np.array([class_id], dtype='U20')\n",
    "    else:\n",
    "        labels = np.concatenate((labels, [class_id]), axis=0, dtype='U20')\n",
    "\n",
    "np.save('/names.npy', labels)\n",
    "\n",
    "datatype = 'train'\n",
    "# DLiske (I added b/c it seems like we shouldn't overwrite the avg embeddings if we're doing this on Test...)\n",
    "if datatype == 'train': # DLiske\n",
    "    # save avg embedding per class to be used as visualization and for further processing\n",
    "    np.savetxt(f\"vecs-conc-{base_model}.tsv\", mean_values, delimiter=\"\\t\")\n",
    "    #np.savetxt(f\"meta-conc-{base_model}.tsv\", labels, fmt=\"%i\", delimiter=\"\\t\")\n",
    "    np.savetxt(f\"meta-conc-{base_model}.tsv\", labels, fmt=\"%s\", delimiter=\"\\t\") #D LISKE\n",
    "    # np.savetxt(\n",
    "    #     f\"emb_space.csv\", np.concatenate((mean_values, labels), axis=1), delimiter=\"\\t\"\n",
    "    # )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f41121",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T07:20:52.398059Z",
     "start_time": "2022-09-26T07:20:52.398053Z"
    }
   },
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3888e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T07:20:52.399507Z",
     "start_time": "2022-09-26T07:20:52.399501Z"
    }
   },
   "outputs": [],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61805dd",
   "metadata": {},
   "source": [
    "# Generate Siamese Embedded Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426c6d04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-15T10:29:01.342084Z",
     "start_time": "2022-09-15T10:29:01.231756Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # generate_siamese_emb_space.py\n",
    "\n",
    "# import io\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import tensorflow_datasets as tfds\n",
    "# from absl import app, flags\n",
    "# from absl.flags import FLAGS\n",
    "\n",
    "# from data.data_generator import DataGenerator\n",
    "# from model.siamese.model_generator import create_model, base_models\n",
    "# from model.siamese.config import cfg\n",
    "\n",
    "# # flags.DEFINE_string(\n",
    "# #     \"weights\",\n",
    "# #     \"siam-118-0.0001-1.0a_0.0633.h5\",\n",
    "# #     \"weights name\",\n",
    "# # )\n",
    "\n",
    "# # flags.DEFINE_string(\n",
    "# #     \"datatype\",\n",
    "# #     \"train\",\n",
    "# #     \"weights name\",\n",
    "# # )\n",
    "# datatype = 'train' #D LISKE\n",
    "# weights = 'siam-121-0.0001-_0.0000.h5' #D LISKE\n",
    "\n",
    "# WEIGHTS_DIR = \"model/siamese/weights\"\n",
    "\n",
    "# base_model = list(base_models.keys())[0]  # MobileNetV2, ResNet101V2, EfficientNetB5\n",
    "\n",
    "\n",
    "# ### def main(_argv):\n",
    "# #model = create_model(base_model=base_model)\n",
    "# # if FLAGS.datatype != \"train\" and FLAGS.datatype != \"test\":\n",
    "# #     FLAGS.datatype = \"train\"\n",
    "\n",
    "# if datatype != \"train\" and datatype != \"test\":\n",
    "#     datatype = \"train\"\n",
    "\n",
    "\n",
    "# #model.load_weights(f\"{WEIGHTS_DIR}/{base_model}/{FLAGS.weights}\")\n",
    "# #model.load_weights(f\"{WEIGHTS_DIR}/{base_model}/{weights}\")\n",
    "\n",
    "\n",
    "# ds_generator = DataGenerator(\n",
    "#     file_ext=[\"png\", \"jpg\"],\n",
    "#     #folder_path=f\"data/filter_aug/{FLAGS.datatype}\",\n",
    "#     #folder_path=f\"data/filter_aug/{datatype}\",\n",
    "#     folder_path=f\"data/filter_aug/train\",\n",
    "#     exclude_aug=True,\n",
    "#     step_size=1,\n",
    "# )\n",
    "\n",
    "# dataset = ds_generator.get_dataset()\n",
    "\n",
    "# results = model.predict(dataset)\n",
    "\n",
    "# # save pure results (embedding) and create meta mapping for each row (visualization files)\n",
    "# #np.savetxt(f\"vecs-{FLAGS.datatype}-{base_model}.tsv\", results, delimiter=\"\\t\")\n",
    "# np.savetxt(f\"vecs-{datatype}-{base_model}.tsv\", results, delimiter=\"\\t\")\n",
    "# #out_m = io.open(f\"meta-{FLAGS.datatype}-{base_model}.tsv\", \"w\", encoding=\"utf-8\")\n",
    "# out_m = io.open(f\"meta-{datatype}-{base_model}.tsv\", \"w\")#, encoding=\"utf-8\")\n",
    "# for img, labels in tfds.as_numpy(dataset):\n",
    "#     [out_m.write(str(x) + \"\\n\") for x in labels]\n",
    "# out_m.close()\n",
    "\n",
    "# # merge all embeddings per class\n",
    "# per_class = {}\n",
    "# idx = 0\n",
    "# for img, labels in tfds.as_numpy(dataset):\n",
    "#     for class_id in labels:\n",
    "#         if class_id not in per_class:\n",
    "#             per_class[class_id] = []\n",
    "#         per_class[class_id].append(results[idx])\n",
    "#         idx += 1\n",
    "\n",
    "# mean_values = None\n",
    "# labels = None\n",
    "# # calculate average value for each class\n",
    "# for class_id, values in per_class.items():\n",
    "#     #print(\"CLASS_ID\", class_id)#, values)\n",
    "#     matrix = np.array(values)\n",
    "#     mean_val = np.mean(matrix, axis=0)\n",
    "#     if mean_values is None:\n",
    "#         mean_values = np.array([mean_val])\n",
    "#     else:\n",
    "#         mean_values = np.concatenate((mean_values, np.array([mean_val])), axis=0)\n",
    "#     if labels is None:\n",
    "#         labels = np.array([class_id], dtype='U20')\n",
    "#     else:\n",
    "#         labels = np.concatenate((labels, [class_id]), axis=0, dtype='U20')\n",
    "              \n",
    "# # DLiske (I added b/c it seems like we shouldn't overwrite the avg embeddings if we're doing this on Test...)\n",
    "# if datatype == 'train': # DLiske\n",
    "#     # save avg embedding per class to be used as visualization and for further processing\n",
    "#     np.savetxt(f\"vecs-conc-{base_model}.tsv\", mean_values, delimiter=\"\\t\")\n",
    "#     #np.savetxt(f\"meta-conc-{base_model}.tsv\", labels, fmt=\"%i\", delimiter=\"\\t\")\n",
    "#     np.savetxt(f\"meta-conc-{base_model}.tsv\", labels, fmt=\"%s\", delimiter=\"\\t\") #D LISKE\n",
    "#     # np.savetxt(\n",
    "#     #     f\"emb_space.csv\", np.concatenate((mean_values, labels), axis=1), delimiter=\"\\t\"\n",
    "#     # )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e194d24a",
   "metadata": {},
   "source": [
    "# Test Siamese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ced1f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T07:37:24.440500Z",
     "start_time": "2022-09-26T07:37:18.997415Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# test_siamese.py\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "from absl import app, flags\n",
    "from absl.flags import FLAGS\n",
    "\n",
    "from data.data_generator import DataGenerator\n",
    "#from data.names import names\n",
    "from helpers.score_processing import cm_analysis, classification_report_latex\n",
    "from model.siamese.model_generator import create_model, base_models\n",
    "from model.siamese.config import cfg\n",
    "from data.siamese_evaluator import SiameseEvaluator\n",
    "\n",
    "names = np.load('/Users/debbieliske/Documents/CodingProjects/farm-animal-tracking-main/names.npy')\n",
    "\n",
    "# flags.DEFINE_string(\n",
    "#     \"weights\",\n",
    "#     \"siam-147-0.001-block3c_add_0.2488.h5\",\n",
    "#     \"weights name\",\n",
    "# )\n",
    "\n",
    "# flags.DEFINE_string(\n",
    "#     \"datatype\",\n",
    "#     \"train\",\n",
    "#     \"weights name\",\n",
    "# )\n",
    "\n",
    "# flags.DEFINE_string(\n",
    "#     \"vectors\",\n",
    "#     \"model/siamese/vectors/vecs-conc-EfficientNetB5.tsv\",\n",
    "#     \"path to vectors tsv\",\n",
    "# )\n",
    "\n",
    "# flags.DEFINE_string(\n",
    "#     \"meta\",\n",
    "#     \"model/siamese/vectors/meta-conc-EfficientNetB5.tsv\",\n",
    "#     \"path to meta tsv\",\n",
    "# )\n",
    "\n",
    "WEIGHTS_DIR = \"model/siamese/weights\"\n",
    "#weights = 'siam-121-0.0001-_0.0000.h5' # DLiske\n",
    "vectors = 'vecs-conc-MobileNetV2.tsv' # DLiske\n",
    "meta = 'meta-conc-MobileNetV2.tsv'  # DLiske\n",
    "\n",
    "#base_model = list(base_models.keys())[0]\n",
    "# flags.DEFINE_string('target', './crop_images/5.jpg', 'path to input image')\n",
    "# flags.DEFINE_string('source', './crop_images/1.jpg', 'path to input image')\n",
    "\n",
    "\n",
    "def generate_test_dir(basemodel):\n",
    "    test_dir = os.path.join(\n",
    "        \"experiments\", \"siamese\", basemodel\n",
    "    )\n",
    "\n",
    "    if not os.path.isdir(test_dir):\n",
    "        os.mkdir(test_dir)\n",
    "\n",
    "    out_dir = os.path.join(\n",
    "        test_dir, datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    )\n",
    "\n",
    "    if not os.path.isdir(out_dir):\n",
    "        os.mkdir(out_dir)\n",
    "\n",
    "    return out_dir\n",
    "\n",
    "\n",
    "#def main(_argv):\n",
    "out_dir = generate_test_dir(base_model)\n",
    "\n",
    "#model = create_model(base_model=base_model)\n",
    "#model.load_weights(f\"{WEIGHTS_DIR}/{base_model}/{FLAGS.weights}\")\n",
    "#model.load_weights(f\"{WEIGHTS_DIR}/{base_model}/{weights}\")\n",
    "ds_generator = DataGenerator(\n",
    "    file_ext=[\"png\", \"jpg\"],\n",
    "    folder_path=test_data_folder,\n",
    "    exclude_aug=True,\n",
    "    step_size=1,\n",
    ")\n",
    "\n",
    "# images = ds_generator.__getitem__(1)\n",
    "# print(images[0].shape)\n",
    "# print(images[1])\n",
    "\n",
    "\n",
    "evaluator = SiameseEvaluator(model=model, dataset=ds_generator.org_images)\n",
    "#evaluator.set_avg_vectors(FLAGS.vectors, FLAGS.meta)\n",
    "evaluator.set_avg_vectors(vectors, meta)\n",
    "conf_matrix, class_report, preds = evaluator.run_evaluation(compare_type=\"individual\")\n",
    "print(conf_matrix.shape)\n",
    "display(pd.DataFrame(class_report).transpose().round(2))\n",
    "cm_analysis(conf_matrix, names, filename=os.path.join(out_dir, f\"conf_matrix_{base_model}.png\"))\n",
    "#classification_report_latex(class_report, filename=os.path.join(out_dir, f\"class_report_{base_model}.txt\"))\n",
    "\n",
    "### DLiske: I added this b/c it seems we'd want to create embeddings for the test as well....?\n",
    "# save pure results (embedding) and create meta mapping for each row (visualization files)\n",
    "#np.savetxt(f\"vecs-{FLAGS.datatype}-{base_model}.tsv\", results, delimiter=\"\\t\")\n",
    "# datatype = 'test'\n",
    "# np.savetxt(f\"vecs-{datatype}-{base_model}.tsv\", results, delimiter=\"\\t\")\n",
    "# #out_m = io.open(f\"meta-{FLAGS.datatype}-{base_model}.tsv\", \"w\", encoding=\"utf-8\")\n",
    "# out_m = io.open(f\"meta-{datatype}-{base_model}.tsv\", \"w\")#, encoding=\"utf-8\")\n",
    "# for img, labels in tfds.as_numpy(dataset):\n",
    "#     [out_m.write(str(x) + \"\\n\") for x in labels]\n",
    "# out_m.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f09bff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-20T16:45:51.615224Z",
     "start_time": "2022-09-20T16:45:51.550263Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01610d5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-20T16:46:53.387748Z",
     "start_time": "2022-09-20T16:46:44.546922Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "model_input_dir = '/Users/debbieliske/Documents/CodingProjects/farm-animal-tracking-main/model_input_dir/'\n",
    "train_dir = '/Users/debbieliske/Documents/CodingProjects/farm-animal-tracking-main/data/filter_aug/train/'\n",
    "\n",
    "actual_files = os.listdir(os.path.join(train_dir, actual_class))\n",
    "predicted_files = os.listdir(os.path.join(train_dir, predicted_class))\n",
    "\n",
    "for i in range(len(preds)):\n",
    "    actual_class = preds[i][0]\n",
    "    predicted_class = preds[i][1]\n",
    "    if actual_class != predicted_class:\n",
    "        actual_files = os.listdir(os.path.join(train_dir, actual_class))\n",
    "        predicted_files = os.listdir(os.path.join(train_dir, predicted_class ))\n",
    "        print(\"Actual Individual\", actual_class)\n",
    "        print(\"Predicted Individual\", predicted_class)\n",
    "        actual = plt.imread(preds[i][2])\n",
    "        plt.imshow(actual, cmap='gray')\n",
    "        plt.title(os.path.basename(filename), fontsize=15)\n",
    "        plt.axis('off')\n",
    "        im1 = plt.imread(os.path.join(train_dir, actual_class, actual_files[0]))\n",
    "        im2 = plt.imread(os.path.join(train_dir, actual_class, actual_files[1]))\n",
    "        im3 = plt.imread(os.path.join(train_dir, actual_class, actual_files[2]))\n",
    "        im4 = plt.imread(os.path.join(train_dir, actual_class, actual_files[3]))\n",
    "        im5 = plt.imread(os.path.join(train_dir, predicted_class, predicted_files[0]))\n",
    "        im6 = plt.imread(os.path.join(train_dir, predicted_class, predicted_files[1]))\n",
    "        im7 = plt.imread(os.path.join(train_dir, predicted_class, predicted_files[2]))\n",
    "        im8 = plt.imread(os.path.join(train_dir, predicted_class, predicted_files[3]))\n",
    "\n",
    "        fig = plt.figure(figsize=(40, 10))\n",
    "        #fig.suptitle(title, fontsize=10)\n",
    "        grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                         nrows_ncols=(1,4),  # creates 2x2 grid of axes\n",
    "                         #axes_pad=0.001,  # pad between axes in inch.\n",
    "                         )\n",
    "        for ax, im, filename in zip(grid, [im1, im2, im3, im4], actual_files):\n",
    "            # Iterating over the grid returns the Axes.\n",
    "            ax.imshow(im, cmap='gray')\n",
    "            ax.set_title(\"Actual: \\n\" + os.path.basename(filename), fontsize=30)\n",
    "            ax.axis('off')\n",
    "\n",
    "        fig = plt.figure(figsize=(40, 10))    \n",
    "        grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                     nrows_ncols=(1,4),  # creates 2x2 grid of axes\n",
    "                     #axes_pad=0.001,  # pad between axes in inch.\n",
    "                     )          \n",
    "        for ax, im, filename in zip(grid, [im5, im6, im7, im8], predicted_files):\n",
    "            # Iterating over the grid returns the Axes.\n",
    "            ax.imshow(im, cmap='gray')\n",
    "            ax.set_title(\"Predicted: \\n\" + os.path.basename(filename), fontsize=30)\n",
    "            ax.axis('off')\n",
    "\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce1bc8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-21T12:23:26.671270Z",
     "start_time": "2022-09-21T12:23:26.162590Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "IMG_SIZE = (160, 160)\n",
    "IMG_SHAPE = IMG_SIZE + (3,)\n",
    "tmp = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746813b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-21T12:23:33.711626Z",
     "start_time": "2022-09-21T12:23:33.564277Z"
    }
   },
   "outputs": [],
   "source": [
    "tmp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053108fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-21T12:24:33.212818Z",
     "start_time": "2022-09-21T12:24:32.216670Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(\n",
    "    tmp,\n",
    "    to_file=f\"assets/tmp_model_fig.png\",\n",
    "    show_shapes=True,\n",
    "    expand_nested=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3363388",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}